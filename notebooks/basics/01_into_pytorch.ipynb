{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/projects/pytorch-paper-training/venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 10 próbek, każda o 2 cechach\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [2.0, 1.0],\n",
    "    [2.0, 3.0],\n",
    "    [3.0, 5.0],\n",
    "    [1.0, 1.0],\n",
    "    [2.0, 2.0],\n",
    "    [3.0, 3.0],\n",
    "    [4.0, 4.0],\n",
    "    [5.0, 5.0],\n",
    "    [6.0, 6.0],\n",
    "])\n",
    "\n",
    "# target: jedna liczba dla każdej próbki (np. wynik regresji)\n",
    "y_true = torch.tensor([\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [1.5],\n",
    "    [2.5],\n",
    "    [0.5],\n",
    "    [1.0],\n",
    "    [1.5],\n",
    "    [2.0],\n",
    "    [2.5],\n",
    "    [3.0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn((2,1), requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7531, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "loss = F.mse_loss(y_pred, y_true)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-18.0350],\n",
      "        [-20.5561]])\n",
      "tensor([-4.3451])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.9546239376068115\n",
      "Epoch 1, Loss: 1.4283095598220825\n",
      "Epoch 2, Loss: 1.0224456787109375\n",
      "Epoch 3, Loss: 0.9104748964309692\n",
      "Epoch 4, Loss: 0.8756281733512878\n",
      "Epoch 5, Loss: 0.8610572814941406\n",
      "Epoch 6, Loss: 0.8518519401550293\n",
      "Epoch 7, Loss: 0.8441020846366882\n",
      "Epoch 8, Loss: 0.8367816805839539\n",
      "Epoch 9, Loss: 0.8296209573745728\n",
      "Epoch 10, Loss: 0.8225491642951965\n",
      "Epoch 11, Loss: 0.815546989440918\n",
      "Epoch 12, Loss: 0.808609127998352\n",
      "Epoch 13, Loss: 0.8017333745956421\n",
      "Epoch 14, Loss: 0.7949195504188538\n",
      "Epoch 15, Loss: 0.7881661653518677\n",
      "Epoch 16, Loss: 0.7814731597900391\n",
      "Epoch 17, Loss: 0.7748397588729858\n",
      "Epoch 18, Loss: 0.7682653665542603\n",
      "Epoch 19, Loss: 0.7617498636245728\n",
      "Epoch 20, Loss: 0.7552921175956726\n",
      "Epoch 21, Loss: 0.7488919496536255\n",
      "Epoch 22, Loss: 0.7425488233566284\n",
      "Epoch 23, Loss: 0.7362619638442993\n",
      "Epoch 24, Loss: 0.7300311923027039\n",
      "Epoch 25, Loss: 0.72385573387146\n",
      "Epoch 26, Loss: 0.7177351713180542\n",
      "Epoch 27, Loss: 0.7116689682006836\n",
      "Epoch 28, Loss: 0.7056566476821899\n",
      "Epoch 29, Loss: 0.6996976733207703\n",
      "Epoch 30, Loss: 0.6937918066978455\n",
      "Epoch 31, Loss: 0.6879380941390991\n",
      "Epoch 32, Loss: 0.6821364164352417\n",
      "Epoch 33, Loss: 0.6763862371444702\n",
      "Epoch 34, Loss: 0.6706868410110474\n",
      "Epoch 35, Loss: 0.6650381088256836\n",
      "Epoch 36, Loss: 0.6594393253326416\n",
      "Epoch 37, Loss: 0.6538901925086975\n",
      "Epoch 38, Loss: 0.6483901143074036\n",
      "Epoch 39, Loss: 0.6429386734962463\n",
      "Epoch 40, Loss: 0.6375354528427124\n",
      "Epoch 41, Loss: 0.6321800947189331\n",
      "Epoch 42, Loss: 0.6268720626831055\n",
      "Epoch 43, Loss: 0.6216109395027161\n",
      "Epoch 44, Loss: 0.6163961887359619\n",
      "Epoch 45, Loss: 0.6112276315689087\n",
      "Epoch 46, Loss: 0.6061046719551086\n",
      "Epoch 47, Loss: 0.6010268926620483\n",
      "Epoch 48, Loss: 0.5959940552711487\n",
      "Epoch 49, Loss: 0.5910054445266724\n",
      "Epoch 50, Loss: 0.5860610008239746\n",
      "Epoch 51, Loss: 0.5811599493026733\n",
      "Epoch 52, Loss: 0.576302170753479\n",
      "Epoch 53, Loss: 0.5714872479438782\n",
      "Epoch 54, Loss: 0.5667146444320679\n",
      "Epoch 55, Loss: 0.5619840025901794\n",
      "Epoch 56, Loss: 0.557295024394989\n",
      "Epoch 57, Loss: 0.5526473522186279\n",
      "Epoch 58, Loss: 0.5480405688285828\n",
      "Epoch 59, Loss: 0.5434741377830505\n",
      "Epoch 60, Loss: 0.5389479398727417\n",
      "Epoch 61, Loss: 0.5344616174697876\n",
      "Epoch 62, Loss: 0.53001469373703\n",
      "Epoch 63, Loss: 0.5256066918373108\n",
      "Epoch 64, Loss: 0.5212373733520508\n",
      "Epoch 65, Loss: 0.5169064402580261\n",
      "Epoch 66, Loss: 0.5126135945320129\n",
      "Epoch 67, Loss: 0.5083582997322083\n",
      "Epoch 68, Loss: 0.5041403770446777\n",
      "Epoch 69, Loss: 0.49995937943458557\n",
      "Epoch 70, Loss: 0.4958150386810303\n",
      "Epoch 71, Loss: 0.4917069971561432\n",
      "Epoch 72, Loss: 0.48763489723205566\n",
      "Epoch 73, Loss: 0.4835984706878662\n",
      "Epoch 74, Loss: 0.47959738969802856\n",
      "Epoch 75, Loss: 0.4756312966346741\n",
      "Epoch 76, Loss: 0.47169989347457886\n",
      "Epoch 77, Loss: 0.46780282258987427\n",
      "Epoch 78, Loss: 0.46393996477127075\n",
      "Epoch 79, Loss: 0.46011075377464294\n",
      "Epoch 80, Loss: 0.45631489157676697\n",
      "Epoch 81, Loss: 0.45255231857299805\n",
      "Epoch 82, Loss: 0.44882264733314514\n",
      "Epoch 83, Loss: 0.4451254904270172\n",
      "Epoch 84, Loss: 0.4414604604244232\n",
      "Epoch 85, Loss: 0.43782752752304077\n",
      "Epoch 86, Loss: 0.43422627449035645\n",
      "Epoch 87, Loss: 0.43065643310546875\n",
      "Epoch 88, Loss: 0.4271175265312195\n",
      "Epoch 89, Loss: 0.4236096441745758\n",
      "Epoch 90, Loss: 0.42013224959373474\n",
      "Epoch 91, Loss: 0.41668516397476196\n",
      "Epoch 92, Loss: 0.4132680892944336\n",
      "Epoch 93, Loss: 0.40988069772720337\n",
      "Epoch 94, Loss: 0.40652281045913696\n",
      "Epoch 95, Loss: 0.4031940996646881\n",
      "Epoch 96, Loss: 0.3998942971229553\n",
      "Epoch 97, Loss: 0.3966232240200043\n",
      "Epoch 98, Loss: 0.39338061213493347\n",
      "Epoch 99, Loss: 0.39016616344451904\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = x @ w + b\n",
    "    loss = F.mse_loss(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
